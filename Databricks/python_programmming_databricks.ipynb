{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d418e83-ac66-4f10-9ab5-8c3e4976b5b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n2\n3\n4\n5\n\nSum of numbers from 1 to 10:\nSum: 55\n1 x 1 = 1\t1 x 2 = 2\t1 x 3 = 3\t1 x 4 = 4\t1 x 5 = 5\t\n2 x 1 = 2\t2 x 2 = 4\t2 x 3 = 6\t2 x 4 = 8\t2 x 5 = 10\t\n3 x 1 = 3\t3 x 2 = 6\t3 x 3 = 9\t3 x 4 = 12\t3 x 5 = 15\t\n4 x 1 = 4\t4 x 2 = 8\t4 x 3 = 12\t4 x 4 = 16\t4 x 5 = 20\t\n5 x 1 = 5\t5 x 2 = 10\t5 x 3 = 15\t5 x 4 = 20\t5 x 5 = 25\t\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Task 1: FOR Loop\n",
    "# Iterate through a list and print elements\n",
    "numbers = [1, 2, 3, 4, 5]\n",
    "for num in numbers:\n",
    "    print(num)\n",
    "\n",
    "# Calculate the sum of numbers within a range\n",
    "print(\"\\nSum of numbers from 1 to 10:\")\n",
    "total = 0\n",
    "for num in range(1, 11):  # Numbers from 1 to 10\n",
    "    total += num\n",
    "print(\"Sum:\", total)\n",
    "\n",
    "# Generate a multiplication table using nested loops\n",
    "for i in range(1, 6):\n",
    "    for j in range(1, 6):\n",
    "        print(f\"{i} x {j} = {i * j}\", end=\"\\t\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fb87aa0-6d21-4baf-b2c6-b187d4b0bacb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if a number is positive, negative, or zero:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Enter a number:  7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 is Positive\n"
     ]
    }
   ],
   "source": [
    "# Check if a number is positive, negative, or zero (with user input)\n",
    "print(\"Check if a number is positive, negative, or zero:\")\n",
    "\n",
    "# Ask user for input and convert it to an integer\n",
    "num = int(input(\"Enter a number: \"))\n",
    "\n",
    "# Check conditions\n",
    "if num > 0:\n",
    "    print(f\"{num} is Positive\")\n",
    "elif num < 0:\n",
    "    print(f\"{num} is Negative\")\n",
    "else:\n",
    "    print(f\"{num} is Zero\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8c3b664-a0a7-465b-9403-1c4f41fa3a9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Enter the first number:  34"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Enter the second number:  43"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 is the larger number.\n"
     ]
    }
   ],
   "source": [
    "num1 = int(input(\"Enter the first number: \"))\n",
    "num2 = int(input(\"Enter the second number: \"))\n",
    "\n",
    "if num1 > num2:\n",
    "    print(f\"{num1} is the larger number.\")\n",
    "elif num1 < num2:\n",
    "    print(f\"{num2} is the larger number.\")\n",
    "else:\n",
    "    print(\"Both numbers are equal.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5027a96c-c658-4400-ab57-714d1ae7da45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Enter the score:  88"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grade B\n"
     ]
    }
   ],
   "source": [
    "score = int(input(\"Enter the score: \"))\n",
    "\n",
    "if score >= 90:\n",
    "    print(\"Grade A\")\n",
    "elif score >= 80:\n",
    "    print(\"Grade B\")\n",
    "elif score >= 70:\n",
    "    print(\"Grade C\")\n",
    "elif score >= 60:\n",
    "    print(\"Grade D\")\n",
    "else:\n",
    "    print(\"Grade F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf4f1e29-82d7-4d27-8ca7-f02a8e97505d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n1\n2\n3\n4\nFound 5! Exiting loop...\n"
     ]
    }
   ],
   "source": [
    "for number in range(10):\n",
    "    if number == 5:\n",
    "        print(\"Found 5! Exiting loop...\")\n",
    "        break  # Exits the loop when the number is 5\n",
    "    print(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62b13ba5-6413-494f-875c-0291d388f8af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n1\n2\n3\n4\n6\n7\n8\n9\n"
     ]
    }
   ],
   "source": [
    "for number in range(10):\n",
    "    if number == 5:\n",
    "        pass  # Skips the number 5, but continues the loop\n",
    "    else:\n",
    "        print(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b50de0a8-3187-4f98-8db5-cf0ce759673a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n3\n5\nFound 7! Exiting loop...\n"
     ]
    }
   ],
   "source": [
    "for number in range(10):\n",
    "    if number % 2 == 0:\n",
    "        pass  # Skips even numbers\n",
    "    elif number == 7:\n",
    "        print(\"Found 7! Exiting loop...\")\n",
    "        break  # Exits the loop when the number is 7\n",
    "    else:\n",
    "        print(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d983546-848c-4f58-a004-ad8739c09f27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Enter a number to find its factorial:  25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The factorial of 25 is 15511210043330985984000000\n"
     ]
    }
   ],
   "source": [
    "def factorial(n):\n",
    "    if n == 0 or n == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * factorial(n - 1)\n",
    "\n",
    "# Example usage\n",
    "num = int(input(\"Enter a number to find its factorial: \"))\n",
    "print(f\"The factorial of {num} is {factorial(num)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67c92b05-79bf-4849-8ed4-6e9c69480e44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Enter a string to reverse:  sad"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reversed string is: das\n"
     ]
    }
   ],
   "source": [
    "def reverse_string(s):\n",
    "    return s[::-1]\n",
    "\n",
    "# Example usage\n",
    "input_string = input(\"Enter a string to reverse: \")\n",
    "print(f\"The reversed string is: {reverse_string(input_string)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "148e3f0e-c69f-4e87-bbcb-53d72e763947",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Enter a list of numbers separated by spaces:  2 3 4 55 67 65 54"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum value in the list is: 67\n"
     ]
    }
   ],
   "source": [
    "def find_max(numbers):\n",
    "    return max(numbers)\n",
    "\n",
    "# Example usage\n",
    "number_list = [int(x) for x in input(\"Enter a list of numbers separated by spaces: \").split()]\n",
    "print(f\"The maximum value in the list is: {find_max(number_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b532b283-09b3-45cd-95bc-b4fcfbec502c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n|dummy|current_date|\n+-----+------------+\n|dummy|  2025-03-16|\n+-----+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "# Get the current date\n",
    "df = spark.createDataFrame([(\"dummy\",)], [\"dummy\"])  # Example dataframe\n",
    "df.withColumn(\"current_date\", current_date()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47983a46-d48c-403c-84fa-cfb5251050ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+-----+----+\n|date_column|day|month|year|\n+-----------+---+-----+----+\n| 2025-03-16| 16|    3|2025|\n| 2024-12-25| 25|   12|2024|\n+-----------+---+-----+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dayofmonth, month, year\n",
    "\n",
    "# Sample DataFrame with a date column\n",
    "data = [(\"2025-03-16\",), (\"2024-12-25\",)]\n",
    "df = spark.createDataFrame(data, [\"date_column\"])\n",
    "\n",
    "# Extract day, month, and year\n",
    "df = df.withColumn(\"day\", dayofmonth(\"date_column\")) \\\n",
    "       .withColumn(\"month\", month(\"date_column\")) \\\n",
    "       .withColumn(\"year\", year(\"date_column\"))\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2210e19c-f368-4f01-95ce-00a9334126de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+-----+----+----------+\n|date_column|day|month|year|  new_date|\n+-----------+---+-----+----+----------+\n| 2025-03-16| 16|    3|2025|2025-03-21|\n| 2024-12-25| 25|   12|2024|2024-12-30|\n+-----------+---+-----+----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add\n",
    "\n",
    "# Add 5 days to the date_column\n",
    "df = df.withColumn(\"new_date\", date_add(\"date_column\", 5))\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68c0e1fd-87d3-4f18-bf71-e47ab3cce291",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n| ID|   Name|Age|\n+---+-------+---+\n|  2|  Alice| 30|\n|  4|Charlie| 35|\n+---+-------+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [(1, \"John\", 25), (2, \"Alice\", 30), (3, \"Bob\", 22), (4, \"Charlie\", 35)]\n",
    "df = spark.createDataFrame(data, [\"ID\", \"Name\", \"Age\"])\n",
    "\n",
    "# Filter rows where Age is greater than 25\n",
    "filtered_df = df.filter(col(\"Age\") > 25)\n",
    "filtered_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ec42e52-2522-4033-8c02-8cbd76f43a82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n| ID|   Name|Age|\n+---+-------+---+\n|  2|  Alice| 30|\n|  4|Charlie| 35|\n+---+-------+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter rows where Age is greater than 25 and Name is not \"Bob\"\n",
    "filtered_df = df.filter((col(\"Age\") > 25) & (col(\"Name\") != \"Bob\"))\n",
    "filtered_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92c681ee-15fa-41ce-9f4c-aba9758c0c81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n| ID|Name|Age|\n+---+----+---+\n|  1|John| 25|\n+---+----+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where Name is \"John\"\n",
    "filtered_df = df.filter(col(\"Name\") == \"John\")\n",
    "filtered_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77479c0e-6f1d-41e4-8759-82b287abc879",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+\n| ID|Value1|Value2|Sum|\n+---+------+------+---+\n|  1|    10|    20| 30|\n|  2|    15|    30| 45|\n|  3|    10|    25| 35|\n|  4|    20|    40| 60|\n+---+------+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [(1, 10, 20), (2, 15, 30), (3, 10, 25), (4, 20, 40)]\n",
    "df = spark.createDataFrame(data, [\"ID\", \"Value1\", \"Value2\"])\n",
    "\n",
    "# Create a new column based on arithmetic operations (Value1 + Value2)\n",
    "df = df.withColumn(\"Sum\", col(\"Value1\") + col(\"Value2\"))\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7749635f-f228-4f08-a788-297d5ed1b886",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+--------+\n| ID|Value1|Value2|Sum|Category|\n+---+------+------+---+--------+\n|  1|    10|    20| 30|     Low|\n|  2|    15|    30| 45|     Low|\n|  3|    10|    25| 35|     Low|\n|  4|    20|    40| 60|    High|\n+---+------+------+---+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Categorize Age into groups\n",
    "df = df.withColumn(\"Category\", when(col(\"Value1\") > 15, \"High\").otherwise(\"Low\"))\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ea84b59-599e-4a71-8827-0a0f808d5011",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+\n| ID| Name|Upper_Name|\n+---+-----+----------+\n|  1| john|      JOHN|\n|  2|alice|     ALICE|\n|  3|  bob|       BOB|\n+---+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "\n",
    "# Add a column with transformed string values (convert Name to uppercase)\n",
    "data = [(1, \"john\"), (2, \"alice\"), (3, \"bob\")]\n",
    "df = spark.createDataFrame(data, [\"ID\", \"Name\"])\n",
    "\n",
    "df = df.withColumn(\"Upper_Name\", upper(col(\"Name\")))\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30461eaa-39df-4ca9-a243-a7a73161f695",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n| Column1|\n+--------+\n|DATAFLOW|\n|  PYTHON|\n|   SPARK|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Create a DataFrame using a string literal\n",
    "data = [(\"DATAFLOW\",), (\"PYTHON\",), (\"SPARK\",)]\n",
    "df = spark.createDataFrame(data, [\"Column1\"])\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d16f9cd-1841-4a3d-88d5-0825c9fbca36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n| Column1|\n+--------+\n|DATAFLOW|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter rows where Column1 is \"DATAFLOW\"\n",
    "filtered_df = df.filter(col(\"Column1\") == \"DATAFLOW\")\n",
    "filtered_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5870b1a0-475a-4c23-9430-4f5c0255ff27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n| Column1|Constant_Column|\n+--------+---------------+\n|DATAFLOW|       DATAFLOW|\n|  PYTHON|       DATAFLOW|\n|   SPARK|       DATAFLOW|\n+--------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Add a new column with the string literal \"DATAFLOW\"\n",
    "df = df.withColumn(\"Constant_Column\", lit(\"DATAFLOW\"))\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eff11d38-6d43-44da-9b60-5f8d4a9788e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+\n| ID| Name| Department|\n+---+-----+-----------+\n|  1|Alice|         HR|\n|  2|  Bob|Engineering|\n+---+-----+-----------+\n\n+---+-------+-----------+\n| ID|   Name| Department|\n+---+-------+-----------+\n|  1|  Alice|         HR|\n|  2|    Bob|Engineering|\n|  3|Charlie|       null|\n+---+-------+-----------+\n\n+---+-------+-----------+\n| ID|   Name| Department|\n+---+-------+-----------+\n|  1|  Alice|         HR|\n|  2|    Bob|Engineering|\n|  3|Charlie|       null|\n|  4|   null|    Finance|\n+---+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Sample DataFrames\n",
    "data1 = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\")]\n",
    "data2 = [(1, \"HR\"), (2, \"Engineering\"), (4, \"Finance\")]\n",
    "df1 = spark.createDataFrame(data1, [\"ID\", \"Name\"])\n",
    "df2 = spark.createDataFrame(data2, [\"ID\", \"Department\"])\n",
    "\n",
    "# Inner join\n",
    "inner_join_df = df1.join(df2, \"ID\", \"inner\")\n",
    "inner_join_df.show()\n",
    "\n",
    "# Left join\n",
    "left_join_df = df1.join(df2, \"ID\", \"left\")\n",
    "left_join_df.show()\n",
    "\n",
    "# Full Outer join\n",
    "full_outer_join_df = df1.join(df2, \"ID\", \"outer\")\n",
    "full_outer_join_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "606ab51f-9d10-4243-9c91-c3f0f916d3c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+\n| ID|   Name| Department|\n+---+-------+-----------+\n|  1|  Alice|         HR|\n|  2|    Bob|Engineering|\n|  3|Charlie|    Unknown|\n+---+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Left Join with handling null values\n",
    "left_join_df = df1.join(df2, \"ID\", \"left\")\n",
    "\n",
    "# Replace null values with a default value in the Department column\n",
    "left_join_df = left_join_df.fillna({\"Department\": \"Unknown\"})\n",
    "left_join_df.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "python_programmming_databricks",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
